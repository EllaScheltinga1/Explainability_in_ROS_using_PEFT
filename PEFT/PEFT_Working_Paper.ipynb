{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fbed44",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2eecb-6809-47f2-b5f6-662c29de23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets\n",
    "! pip install peft\n",
    "! pip install transformers\n",
    "! pip install transformers[sentencepiece]\n",
    "! pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24e44c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8787c-df44-4707-920f-a9f5e832cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798786c9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d14e3a-a34b-4a9a-a59b-47b2814fee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== LOAD MODEL AND TOKENIZER  ==========\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, MistralForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "token = # Your Hugging Face API token in \"\"\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, device_map='auto')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token, device_map='auto')\n",
    "\n",
    "# =========== PEFT ===========\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Load the PEFT configuration and apply it to the model\n",
    "print(\"Configuring PEFT...\")\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1) # Changed task type to CAUSAL_LM\n",
    "print(\"Getting PEFT model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd48b84",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c55382-86e2-484b-a217-ff2fde9e88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== LOAD CUSTOM DATASET ==========\n",
    "# Load the JSON data file\n",
    "\n",
    "data = # Load your JSON data file path\n",
    "\n",
    "# Convert JSON data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Shuffle the entire dataset before splitting\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "validation_train_split = dataset['train'].train_test_split(test_size=0.2)  # 0.2 of the training dataset for validation\n",
    "dataset[\"validation\"] = validation_train_split[\"test\"]\n",
    "dataset[\"train\"] = validation_train_split[\"train\"]\n",
    "\n",
    "print(\"Length of training dataset:\", len(dataset[\"train\"]))\n",
    "print(\"Length of validation dataset:\", len(dataset[\"validation\"]))\n",
    "print(\"Length of test dataset:\", len(dataset[\"test\"]))\n",
    "print(\"Finished loading dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894c48b",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3badb8-c9f7-4250-b457-7eedb77e7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    questions = example['question']\n",
    "    contexts = example['context']\n",
    "    answers = example['answers']\n",
    "\n",
    "    output_texts = []\n",
    "    for question, context, answer in zip(questions, contexts, answers):\n",
    "        text = f\"### Question: {question}\\n ### Context: {context}\\n  ### Answer: {answer}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    # dataset_text_field=\"answers\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    max_seq_length=1300,\n",
    "    output_dir=\"SFT_Mistral_7B\",\n",
    "    hub_model_id=\"EllaScheltinga/SFT-Mistral-7B\", \n",
    "    push_to_hub=True,\n",
    "    hub_token=token,\n",
    "    logging_steps=100\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "\n",
    ")\n",
    "\n",
    "# Ensure GPU memory management settings\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer to the Hugging Face Hub\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"EllaScheltinga/SFT-Mistral-7B\", token=token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
